{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_nuclear1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4nkB8IqIKS-",
        "colab_type": "text"
      },
      "source": [
        "#### Install all dependencies\n",
        "Everything else should be installed automatically by colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzTCC-An6OMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade tensorflow==1.13.1\n",
        "# !pip install numpy==1.14.6\n",
        "!pip install tifffile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iihbqx6Bkp2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import six\n",
        "import collections\n",
        "from skimage.io import imread\n",
        "import time\n",
        "import os\n",
        "from google.colab import drive\n",
        "import random\n",
        "from scipy.ndimage.interpolation import affine_transform\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "import matplotlib.pyplot as plt\n",
        "import tifffile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKtaF8zRpbBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9AHaCsTkp3E",
        "colab_type": "text"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOgyNaH1NHap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using stacked tiff image as input\n",
        "\n",
        "images_path = '/content/gdrive/My Drive/cropped-stacks/'\n",
        "labels_path = '/content/gdrive/My Drive/cropped-labels-stacks/'\n",
        "\n",
        "validation_rois = ('ROI_2052-5784-112', 'ROI_3588-3972-1')\n",
        "holdout_rois = ('ROI_1656-6756-329', 'ROI_3624-2712-201', 'ROI_1716-7800-517')\n",
        "extension = \".tiff\"\n",
        "\n",
        "import os\n",
        "from glob import glob\n",
        "import zipfile\n",
        "\n",
        "def get_file(file_pattern):\n",
        "  filenames = glob(file_pattern)\n",
        "  if len(filenames) != 0:\n",
        "    return filenames[0]\n",
        "  return \"\"\n",
        "\n",
        "def unzip_zipfile(path):\n",
        "  zipfilename = get_file(path + \"*.zip\")\n",
        "  if zipfilename:\n",
        "    print(\"Unzipping: \", zipfilename)\n",
        "    with zipfile.ZipFile(zipfilename, \"r\") as zip_ref:\n",
        "      zip_ref.extractall(path)\n",
        "    os.remove(zipfilename)\n",
        "\n",
        "unzip_zipfile(images_path)\n",
        "unzip_zipfile(labels_path)\n",
        "\n",
        "rois = [os.path.splitext(filename)[0] for filename in os.listdir(labels_path)]\n",
        "\n",
        "training_rois = [roi for roi in rois\n",
        "                 if roi not in validation_rois\n",
        "                 and roi not in holdout_rois]\n",
        "\n",
        "\n",
        "print(\"Training data:   \", training_rois)\n",
        "print(\"Validation data: \", validation_rois)\n",
        "print(\"Holdout data:    \", holdout_rois)\n",
        "\n",
        "\n",
        "def load_stack_pair(roi):\n",
        "  image = imread(images_path + roi + extension)\n",
        "  label = np.where(imread(labels_path + roi + extension) >= 0.5, 1, 0)   # binary conversion by round-off\n",
        "  return [image, label]\n",
        "\n",
        "\n",
        "def get_random_training_stack():\n",
        "  return load_stack_pair(random.choice(training_rois))\n",
        "\n",
        "\n",
        "def get_random_validation_stack():\n",
        "  return load_stack_pair(random.choice(validation_rois))\n",
        "\n",
        "\n",
        "def check_data():\n",
        "  print(\"Training sets\")\n",
        "  for roi in training_rois:\n",
        "    pair = load_stack_pair(roi)\n",
        "    print(roi, pair[0].shape, pair[1].shape)\n",
        "\n",
        "  print(\"Validation sets\")\n",
        "  for roi in validation_rois:\n",
        "    pair = load_stack_pair(roi)\n",
        "    print(roi, pair[0].shape, pair[1].shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyVU_wTgv6JO",
        "colab_type": "text"
      },
      "source": [
        "**Data augmentation utils**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEf3fwvEv1gH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_matrix_offset_center(matrix, x, y):\n",
        "  o_x = float(x) / 2 + 0.5\n",
        "  o_y = float(y) / 2 + 0.5\n",
        "  offset_matrix = np.array([[1, 0, o_x], [0, 1, o_y], [0, 0, 1]])\n",
        "  reset_matrix = np.array([[1, 0, -o_x], [0, 1, -o_y], [0, 0, 1]])\n",
        "  transform_matrix = np.dot(np.dot(offset_matrix, matrix), reset_matrix)\n",
        "  return transform_matrix\n",
        "\n",
        "\n",
        "def zoom_and_rotate_patch(patch, angle, zoom):\n",
        "  theta = np.deg2rad(angle)\n",
        "  rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n",
        "                              [np.sin(theta), np.cos(theta), 0],\n",
        "                              [0, 0, 1]])\n",
        "  transform_matrix = rotation_matrix\n",
        "\n",
        "  zoom_matrix = np.array([[zoom, 0, 0],\n",
        "                          [0, zoom, 0],\n",
        "                          [0, 0, 1]])\n",
        "\n",
        "  transform_matrix = np.dot(transform_matrix, zoom_matrix)\n",
        "\n",
        "  d, h, w = patch.shape\n",
        "  transform_matrix = transform_matrix_offset_center(transform_matrix, h, w)\n",
        "  patch = np.rollaxis(patch, 0, 0)\n",
        "  final_affine_matrix = transform_matrix[:2, :2]\n",
        "  final_offset = transform_matrix[:2, 2]\n",
        "\n",
        "  channel_images = [\n",
        "    affine_transform(\n",
        "      x_channel,\n",
        "      final_affine_matrix,\n",
        "      final_offset,\n",
        "      order=1,\n",
        "      mode='reflect'\n",
        "    ) for x_channel in patch\n",
        "  ]\n",
        "\n",
        "  patch = np.stack(channel_images, axis=0)\n",
        "  patch = np.rollaxis(patch, 0, 1)\n",
        "\n",
        "  return patch\n",
        "\n",
        "\n",
        "def normalize_batch(batch):\n",
        "  \"\"\" normalize X batch by subbing mean then dividing by std\n",
        "      normalized over the 0 axis (patch-wise mean and std)\n",
        "  \"\"\"\n",
        "  mean = batch.mean(axis=(1, 2, 3), keepdims=True)\n",
        "  std = batch.std(axis=(1, 2, 3), keepdims=True)\n",
        "  batch = (batch - mean) / (std + 0.0001)\n",
        "  return batch\n",
        "\n",
        "\n",
        "sometimes = lambda x: np.random.random() < x  # True x% of the time\n",
        "\n",
        "def augment_and_normalize_batch(image_patches, label_patches):\n",
        "\n",
        "  augmented_image_patches = []\n",
        "  augmented_label_patches = []\n",
        "\n",
        "  for i in range(len(image_patches)):\n",
        "    \n",
        "    img_patch = image_patches[i]\n",
        "    lbl_patch = label_patches[i]\n",
        "    \n",
        "    if sometimes(0.5):  # 50% of the time, rotate or zoom the image\n",
        "      zoom = np.random.normal(loc=1.0, scale=ZOOM_RANGE)\n",
        "      angle = np.random.normal(loc=0.0, scale=ROTATION_RANGE)\n",
        "      img_patch = zoom_and_rotate_patch(img_patch, angle, zoom)\n",
        "      lbl_patch = zoom_and_rotate_patch(lbl_patch, angle, zoom)\n",
        "\n",
        "    if sometimes(0.25):  # 25% of the time, blur OR noise up the image\n",
        "      if sometimes(0.5):\n",
        "        sigma = np.random.normal(loc=0.0, scale=BLUR_RANGE)\n",
        "        img_patch = gaussian_filter(img_patch, sigma=sigma)\n",
        "        # don't blur the segmentation mask\n",
        "      else:\n",
        "        img_patch = img_patch + np.random.normal(loc=0.0, scale=NOISE, size=img_patch.shape)\n",
        "    \n",
        "    if sometimes(0.25):  # 25% of the time, change brightness or contrast\n",
        "      if sometimes(0.5):\n",
        "        img_patch = img_patch + np.random.normal(loc=0.0, scale=BRIGHTNESS_RANGE)\n",
        "      else:\n",
        "        img_patch = img_patch * np.random.normal(loc=1.0, scale=CONTRAST_RANGE)\n",
        "        \n",
        "    augmented_image_patches.append(img_patch)\n",
        "    augmented_label_patches.append(lbl_patch)\n",
        "\n",
        "  image_patches = np.stack(augmented_image_patches, axis=0)\n",
        "  label_patches = np.stack(augmented_label_patches, axis=0)\n",
        "  image_patches = normalize_batch(image_patches)\n",
        "\n",
        "  return image_patches, label_patches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKWKPkXvkp3T",
        "colab_type": "text"
      },
      "source": [
        "#### Data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f4-dxcLkp3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_range(begin, end):\n",
        "  if end > begin:\n",
        "    return np.random.randint(begin, end)\n",
        "  else:\n",
        "    return begin\n",
        "\n",
        "\n",
        "def get_random_batch_corner_coordinates(batch_size, region):\n",
        "  \"\"\" @param region: (Z0, Z1, Y0, Y1, X0, X1) return coordniates in high/low range given\n",
        "      @param batch_size: how many random coordinates to generate?\n",
        "\n",
        "      @return: (batch_size, 3) stacks of random (Z, Y, X) coordinates\n",
        "  \"\"\"\n",
        "  \n",
        "\n",
        "  r = np.array([[get_random_range(region[0], region[1] - PATCH_SHAPE[0]),\n",
        "                 get_random_range(region[2], region[3] - PATCH_SHAPE[1]),\n",
        "                 get_random_range(region[4], region[5] - PATCH_SHAPE[2])\n",
        "                 ] for _ in range(batch_size)])\n",
        "  return r\n",
        "\n",
        "\n",
        "def get_image_patch(image, corner_coordinate):\n",
        "  \"\"\"\n",
        "\n",
        "  :param image:\n",
        "  :param corner_coordinate:\n",
        "  :param patch_shape:\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  patch = image[corner_coordinate[0]:corner_coordinate[0] + PATCH_SHAPE[0],\n",
        "                corner_coordinate[1]:corner_coordinate[1] + PATCH_SHAPE[1],\n",
        "                corner_coordinate[2]:corner_coordinate[2] + PATCH_SHAPE[2]]\n",
        "  return patch\n",
        "\n",
        "\n",
        "def get_train_generator(batch_size):\n",
        "  \"\"\"\n",
        "  :param patch_shape:\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  \n",
        "  while True:\n",
        "    image, label = get_random_training_stack()\n",
        "    rand_rot = random.randint(0, 3)\n",
        "    image = np.rot90(image, k=rand_rot, axes=(1, 2))  # randomly rotate 90\n",
        "    label = np.rot90(label, k=rand_rot, axes=(1, 2))\n",
        "    S = image.shape\n",
        "    region = (0, S[0], 0, S[1], 0, S[2])\n",
        "    corner_coords = get_random_batch_corner_coordinates(batch_size, region)\n",
        "    image_patches = []\n",
        "    label_patches = []\n",
        "    for corner in corner_coords:\n",
        "      image_patch = get_image_patch(image, corner)\n",
        "      image_patches.append(image_patch)\n",
        "      label_patch = get_image_patch(label, corner)\n",
        "      label_patches.append(label_patch)\n",
        "\n",
        "    image_patches = np.moveaxis(np.array(image_patches), 1, 3)  # tf --> (N, W, H, D)\n",
        "    label_patches = np.moveaxis(np.array(label_patches), 1, 3)    \n",
        "    \n",
        "    augmented_imgs, augmented_labels = augment_and_normalize_batch(image_patches, label_patches)\n",
        "    \n",
        "    yield augmented_imgs, augmented_labels\n",
        "\n",
        "def visualise_training_batch():\n",
        "  data_gen = get_train_generator(1)\n",
        "  image_batch,label_batch = next(data_gen)\n",
        "  image = image_batch[0][:,:,0]\n",
        "  label = label_batch[0][:,:,0]\n",
        "  plt.imshow(image, cmap='gray')\n",
        "  plt.imshow(np.ma.masked_where(label == 0, label), vmin=0, vmax=1, cmap='cool', alpha=0.5)\n",
        "  plt.show()\n",
        "\n",
        "#visualise_training_batch()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CAdVcHISGQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_validation_data():\n",
        "  validation_x, validation_y = get_random_validation_stack()\n",
        "\n",
        "  D, H, W = validation_x.shape\n",
        "  region = (0, D, 0, H, 0, W)\n",
        "\n",
        "  validation_x_patches = []\n",
        "  validation_y_patches = []\n",
        "\n",
        "  for i in range(4):\n",
        "    corner_coords = get_random_batch_corner_coordinates(4, region)\n",
        "    for corner in corner_coords:\n",
        "      x_patch = get_image_patch(validation_x, corner)\n",
        "      validation_x_patches.append(x_patch)\n",
        "      y_patch = get_image_patch(validation_y, corner)\n",
        "      validation_y_patches.append(y_patch)\n",
        "\n",
        "  validation_x_patches = np.moveaxis(np.array(validation_x_patches), 1, 3)\n",
        "  validation_y_patches = np.moveaxis(np.array(validation_y_patches), 1, 3)\n",
        "\n",
        "  validation_x_patches = normalize_batch(validation_x_patches)\n",
        "\n",
        "  validation_data = (validation_x_patches, validation_y_patches)\n",
        "  \n",
        "  return validation_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQNpwbaGkp3H",
        "colab_type": "text"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTJcv87Xkp3N",
        "colab_type": "text"
      },
      "source": [
        "**TPU is extremely fast, but unfortunately it's poorly integrated with Keras. Modification of the following callbacks is necessary to enable learning rate decay and training history logging. Maybe this will be fixed with TF 2.0, in which case, replace `ReduceLROnPlateauMODIFIED` with `ReduceLROnPlateau` and `CSVLoggerMODIFIED` with `CSVLogger` in get_callbacks().**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSGCLuSEkp3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Concatenate, MaxPooling2D ,Conv2DTranspose, LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.python.util.tf_export import tf_export\n",
        "from tensorflow.python.keras.callbacks import Callback\n",
        "\n",
        "EPOCHS = 100\n",
        "STEPS_PER_EPOCH = 100\n",
        "BATCH_SIZE = 16\n",
        "PATCH_SHAPE = (12, 256, 256)    # (Z, Y, X) Note: X / Y appear to be used interchangebly\n",
        "NUM_LAYERS = 4\n",
        "START_CH = 32\n",
        "OVERLAP_X_Y = PATCH_SHAPE[1]//4\n",
        "OVERLAP_Z = PATCH_SHAPE[0]//4\n",
        "DROPOUT = 0.3\n",
        "model_name = f\"gdrive/My Drive/models/{NUM_LAYERS}L_{START_CH}ch_{PATCH_SHAPE}_{DROPOUT}DROPOUT\"\n",
        "\n",
        "\n",
        "# DATA AUGMENTATION CONFIGURATION\n",
        "ROTATION_RANGE = 0  # +/- ~90˚\n",
        "ZOOM_RANGE = 0.05 # +/- ~10% zoom\n",
        "CONTRAST_RANGE = 0.1 # +/- ~10% constrast\n",
        "BRIGHTNESS_RANGE = 0.1 # +/- ~10% brightness\n",
        "BLUR_RANGE = 0.2 # blur +/- 2 sigma\n",
        "NOISE = 12  # range of noise +/- 12 brightness\n",
        "\n",
        "\n",
        "# model construction\n",
        "\n",
        "\n",
        "def conv_block(m, dim, bn, res, do=0):\n",
        "  n = BatchNormalization()(m) if bn else m\n",
        "  n = Dropout(do)(n) if do else m\n",
        "  # inception\n",
        "  tower_1 = Conv2D(dim, 1, padding='same', activation='linear')(n)\n",
        "  tower_1 = LeakyReLU(alpha=0.1)(tower_1)\n",
        "  tower_1 = Conv2D(dim, (3, 3), padding='same', activation='linear')(tower_1)\n",
        "  tower_1 = LeakyReLU(alpha=0.1)(tower_1)\n",
        "  tower_2 = Conv2D(dim, (1, 1), padding='same', activation='linear')(n)\n",
        "  tower_2 = LeakyReLU(alpha=0.1)(tower_2)\n",
        "  tower_2 = Conv2D(dim, (5, 5), padding='same', activation='linear')(tower_2)\n",
        "  tower_2 = LeakyReLU(alpha=0.1)(tower_2)\n",
        "  tower_3 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(n)\n",
        "  tower_3 = LeakyReLU(alpha=0.1)(tower_3)\n",
        "  tower_3 = Conv2D(dim, (1, 1), padding='same', activation='linear')(tower_3)\n",
        "  tower_3 = LeakyReLU(alpha=0.1)(tower_3)\n",
        "  n = Concatenate()([tower_1, tower_2, tower_3])\n",
        "\n",
        "  return Concatenate()([m, n]) if res else n\n",
        "\n",
        "\n",
        "def level_block(m, dim, depth, inc, do, bn, mp, up, res):\n",
        "  if depth > 0:\n",
        "    n = conv_block(m, dim, bn, res)\n",
        "    m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
        "    m = level_block(m, int(inc * dim), depth - 1, inc, do, bn, mp, up, res)\n",
        "    if up:\n",
        "      m = UpSampling2D()(m)\n",
        "      m = Conv2D(dim, 2, activation='linear', padding='same')(m)\n",
        "      m = LeakyReLU(alpha=0.1)(m)\n",
        "    else:\n",
        "      m = Conv2DTranspose(dim, 3, strides=2, activation='linear', padding='same')(m)\n",
        "      m = LeakyReLU(alpha=0.1)(m)\n",
        "    n = Concatenate()([n, m])\n",
        "    m = conv_block(n, dim, bn, res)\n",
        "  else:\n",
        "    m = conv_block(m, dim, bn, res, do)\n",
        "  return m\n",
        "\n",
        "\n",
        "def UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2.,\n",
        "         dropout=0.5, batchnorm=True, maxpool=True, upconv=True, residual=True):\n",
        "  i = Input(shape=img_shape)\n",
        "  o = level_block(i, start_ch, depth, inc_rate, dropout, batchnorm, maxpool, upconv, residual)\n",
        "  o = Conv2D(out_ch, 1, activation='sigmoid')(o)\n",
        "  return Model(inputs=i, outputs=o)\n",
        "\n",
        "\n",
        "# loss functions\n",
        "\n",
        "\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    \n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1-dice_coef(y_true, y_pred)\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return binary_crossentropy(y_true, y_pred) + dice_coef_loss(y_true, y_pred)\n",
        "\n",
        "\n",
        "# callbacks\n",
        "\n",
        "\n",
        "@tf_export('keras.callbacks.ReduceLROnPlateau')\n",
        "class ReduceLROnPlateauMODIFIED(Callback):\n",
        "  \"\"\"Reduce learning rate when a metric has stopped improving.\n",
        "\n",
        "  Models often benefit from reducing the learning rate by a factor\n",
        "  of 2-10 once learning stagnates. This callback monitors a\n",
        "  quantity and if no improvement is seen for a 'patience' number\n",
        "  of epochs, the learning rate is reduced.\n",
        "\n",
        "  Example:\n",
        "\n",
        "  ```python\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                patience=5, min_lr=0.001)\n",
        "  model.fit(X_train, Y_train, callbacks=[reduce_lr])\n",
        "  ```\n",
        "\n",
        "  Arguments:\n",
        "      monitor: quantity to be monitored.\n",
        "      factor: factor by which the learning rate will\n",
        "          be reduced. new_lr = lr * factor\n",
        "      patience: number of epochs with no improvement\n",
        "          after which learning rate will be reduced.\n",
        "      verbose: int. 0: quiet, 1: update messages.\n",
        "      mode: one of {auto, min, max}. In `min` mode,\n",
        "          lr will be reduced when the quantity\n",
        "          monitored has stopped decreasing; in `max`\n",
        "          mode it will be reduced when the quantity\n",
        "          monitored has stopped increasing; in `auto`\n",
        "          mode, the direction is automatically inferred\n",
        "          from the name of the monitored quantity.\n",
        "      min_delta: threshold for measuring the new optimum,\n",
        "          to only focus on significant changes.\n",
        "      cooldown: number of epochs to wait before resuming\n",
        "          normal operation after lr has been reduced.\n",
        "      min_lr: lower bound on the learning rate.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               monitor='val_loss',\n",
        "               factor=0.1,\n",
        "               patience=10,\n",
        "               verbose=0,\n",
        "               mode='auto',\n",
        "               min_delta=1e-4,\n",
        "               cooldown=0,\n",
        "               min_lr=0,\n",
        "               **kwargs):\n",
        "    super(ReduceLROnPlateauMODIFIED, self).__init__()\n",
        "\n",
        "    self.monitor = monitor\n",
        "    if factor >= 1.0:\n",
        "      raise ValueError('ReduceLROnPlateau ' 'does not support a factor >= 1.0.')\n",
        "    if 'epsilon' in kwargs:\n",
        "      min_delta = kwargs.pop('epsilon')\n",
        "      logging.warning('`epsilon` argument is deprecated and '\n",
        "                      'will be removed, use `min_delta` instead.')\n",
        "    self.factor = factor\n",
        "    self.min_lr = min_lr\n",
        "    self.min_delta = min_delta\n",
        "    self.patience = patience\n",
        "    self.verbose = verbose\n",
        "    self.cooldown = cooldown\n",
        "    self.cooldown_counter = 0  # Cooldown counter.\n",
        "    self.wait = 0\n",
        "    self.best = 0\n",
        "    self.mode = mode\n",
        "    self.monitor_op = None\n",
        "    self._reset()\n",
        "\n",
        "  def _reset(self):\n",
        "    \"\"\"Resets wait counter and cooldown counter.\n",
        "    \"\"\"\n",
        "    if self.mode not in ['auto', 'min', 'max']:\n",
        "      logging.warning('Learning Rate Plateau Reducing mode %s is unknown, '\n",
        "                      'fallback to auto mode.', self.mode)\n",
        "      self.mode = 'auto'\n",
        "    if (self.mode == 'min' or\n",
        "        (self.mode == 'auto' and 'acc' not in self.monitor)):\n",
        "      self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n",
        "      self.best = np.Inf\n",
        "    else:\n",
        "      self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)\n",
        "      self.best = -np.Inf\n",
        "    self.cooldown_counter = 0\n",
        "    self.wait = 0\n",
        "\n",
        "  def on_train_begin(self, logs=None):\n",
        "    self._reset()\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    logs = logs or {}\n",
        "#     print(\"DEBUG\")\n",
        "#     print(self.model.optimizer.optimizer._opt._lr)\n",
        "#     print(dir(self.model.optimizer.optimizer._opt._lr))\n",
        "#     print(K.get_value(self.model.optimizer.optimizer._opt._lr))\n",
        "    logs['lr'] = self.model.optimizer.optimizer._opt._lr\n",
        "    current = logs.get(self.monitor)\n",
        "    if current is None:\n",
        "      logging.warning('Reduce LR on plateau conditioned on metric `%s` '\n",
        "                      'which is not available. Available metrics are: %s',\n",
        "                      self.monitor, ','.join(list(logs.keys())))\n",
        "\n",
        "    else:\n",
        "      if self.in_cooldown():\n",
        "        self.cooldown_counter -= 1\n",
        "        self.wait = 0\n",
        "\n",
        "      if self.monitor_op(current, self.best):\n",
        "        self.best = current\n",
        "        self.wait = 0\n",
        "      elif not self.in_cooldown():\n",
        "        self.wait += 1\n",
        "        if self.wait >= self.patience:\n",
        "          old_lr = float(self.model.optimizer.optimizer._opt._lr)\n",
        "          if old_lr > self.min_lr:\n",
        "            new_lr = old_lr * self.factor\n",
        "            new_lr = max(new_lr, self.min_lr)\n",
        "            self.model.optimizer.optimizer._opt._lr = new_lr\n",
        "            if self.verbose > 0:\n",
        "              print('\\nEpoch %05d: ReduceLROnPlateau reducing learning '\n",
        "                    'rate to %s.' % (epoch + 1, new_lr))\n",
        "            self.cooldown_counter = self.cooldown\n",
        "            self.wait = 0\n",
        "\n",
        "  def in_cooldown(self):\n",
        "    return self.cooldown_counter > 0\n",
        "\n",
        "\n",
        "@tf_export('keras.callbacks.CSVLogger')\n",
        "class CSVLoggerMODIFIED(Callback):\n",
        "  \"\"\"Callback that streams epoch results to a csv file.\n",
        "\n",
        "  Supports all values that can be represented as a string,\n",
        "  including 1D iterables such as np.ndarray.\n",
        "\n",
        "  Example:\n",
        "\n",
        "  ```python\n",
        "  csv_logger = CSVLogger('training.log')\n",
        "  model.fit(X_train, Y_train, callbacks=[csv_logger])\n",
        "  ```\n",
        "\n",
        "  Arguments:\n",
        "      filename: filename of the csv file, e.g. 'run/log.csv'.\n",
        "      separator: string used to separate elements in the csv file.\n",
        "      append: True: append if file exists (useful for continuing\n",
        "          training). False: overwrite existing file,\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, filename, separator=',', append=False):\n",
        "    self.sep = separator\n",
        "    self.filename = filename\n",
        "    self.append = append\n",
        "    self.writer = None\n",
        "    self.keys = None\n",
        "    self.append_header = True\n",
        "    if six.PY2:\n",
        "      self.file_flags = 'b'\n",
        "      self._open_args = {}\n",
        "    else:\n",
        "      self.file_flags = ''\n",
        "      self._open_args = {'newline': '\\n'}\n",
        "    super(CSVLoggerMODIFIED, self).__init__()\n",
        "\n",
        "  def on_train_begin(self, logs=None):\n",
        "    if self.append:\n",
        "      self.mode = 'a'\n",
        "    else:\n",
        "      self.mode = 'w'\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    \n",
        "    with open(self.filename, 'a') as f:\n",
        "      writer = csv.writer(f, delimiter=',', quotechar=\"'\", quoting=csv.QUOTE_ALL)\n",
        "      writer.writerow([logs['val_loss'], logs['loss'], logs['lr']])\n",
        "\n",
        "  def on_train_end(self, logs=None):\n",
        "    self.writer = None\n",
        "    \n",
        "\n",
        "def get_callbacks(model_name):\n",
        "  callbacks = list()\n",
        "  model_filename = model_name + '.hdf5'\n",
        "  callbacks.append(ModelCheckpoint(\n",
        "    filepath=model_filename,\n",
        "    verbose=1, \n",
        "    monitor='loss',\n",
        "    save_best_only=True,\n",
        "    period=1,\n",
        "    mode='min'\n",
        "  ))\n",
        "\n",
        "  callbacks.append(ReduceLROnPlateauMODIFIED(\n",
        "    monitor='val_loss',\n",
        "    factor=0.75,   # lr = lr*factor\n",
        "    patience=4,  # how many epochs no change\n",
        "    verbose=1\n",
        "  ))\n",
        "\n",
        "  log_filename = model_name + '.log'\n",
        "  callbacks.append(CSVLoggerMODIFIED(log_filename, append=True))\n",
        "    \n",
        "  return callbacks\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZMRmHpYkp3Y",
        "colab_type": "text"
      },
      "source": [
        "#### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqaQJJh4kp3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = UNet(\n",
        "  (PATCH_SHAPE[1], PATCH_SHAPE[2], PATCH_SHAPE[0]),  # bc tf expects (W, H, D)\n",
        "  start_ch=START_CH,\n",
        "  depth=NUM_LAYERS,\n",
        "  out_ch=PATCH_SHAPE[0],\n",
        "  dropout=DROPOUT,\n",
        "  residual=True,\n",
        "  upconv=False\n",
        ")\n",
        "\n",
        "# Optional: Load model from last saved params\n",
        "model_filename = model_name + \".hdf5\"\n",
        "if os.path.exists(model_filename):\n",
        "  print(\"loading model: \" + model_filename)\n",
        "  model = load_model(model_filename, custom_objects={'dice_coef_loss': dice_coef_loss})\n",
        "\n",
        "model.compile(\n",
        "# optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
        "  optimizer=tf.train.AdamOptimizer(learning_rate=0.0005),\n",
        "  loss=dice_coef_loss\n",
        ")\n",
        "\n",
        "\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']  # get TPU address\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "  tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)\n",
        ")\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "  model,\n",
        "  strategy=strategy, \n",
        ")\n",
        "\n",
        "#print(model.summary())\n",
        "\n",
        "tpu_model.fit_generator(\n",
        "  generator=get_train_generator(BATCH_SIZE),\n",
        "  steps_per_epoch=STEPS_PER_EPOCH,\n",
        "  epochs=EPOCHS,\n",
        "  validation_data=get_validation_data(),\n",
        "  callbacks=get_callbacks(model_name),\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWuk-g697RxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}